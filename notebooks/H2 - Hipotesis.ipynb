{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e2fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hipotesis NO 2 a desarrollar en el ejercicio por Irenes\n",
    "# Qué modelos entre Random Forest, Árbol de Decisión Optimizado, clustering KMeans(No supervisado), Modelo LSTM son más efectivos para predecir \n",
    "# si un estudiante pasa, utilizando datos del comportamiento, demografía y desempeño académico del dataset de oulad sobre su compartamiento en la plataforma virtual vle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28282a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Manejo de Datos y Computación Numérica\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "# 2. Visualización de Datos\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 3. Machine Learning - Scikit-learn\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score,\n",
    "    recall_score, f1_score, mean_squared_error,\n",
    "    roc_curve, auc, precision_recall_curve, r2_score, roc_curve, roc_auc_score, average_precision_score, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 4. Deep Learning - TensorFlow/Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d99b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Carga de archivos CSV\n",
    "# Cambia esta ruta si tus archivos están en otra carpeta\n",
    "# Rutas base de carpetas\n",
    "ruta = \"./oulad\"\n",
    "rutaEdaImg = \"./edaimg\"\n",
    "rutaDataPred = \"./datatopredict\"\n",
    "rutaOrdMapping = \"./ordmapping\"\n",
    "rutaResult_Pred = \"./result_pred\"\n",
    "rutaMetrics_models = \"./metrics_models\"\n",
    "\n",
    "# Cargar los archivos\n",
    "student_info = pd.read_csv(os.path.join(ruta, \"studentInfo.csv\"))\n",
    "student_registration = pd.read_csv(os.path.join(ruta, \"studentRegistration.csv\"))\n",
    "student_assessment = pd.read_csv(os.path.join(ruta, \"studentAssessment.csv\"))\n",
    "assessments = pd.read_csv(os.path.join(ruta, \"assessments.csv\"))\n",
    "student_vle = pd.read_csv(os.path.join(ruta, \"studentVle.csv\"))\n",
    "vle = pd.read_csv(os.path.join(ruta, \"vle.csv\"))\n",
    "courses = pd.read_csv(os.path.join(ruta, \"courses.csv\"))\n",
    "\n",
    "# Validación de carga de archivos\n",
    "dataframes = {\n",
    "    \"studentInfo\": student_info,\n",
    "    \"studentRegistration\": student_registration,\n",
    "    \"studentAssessment\": student_assessment,\n",
    "    \"assessments\": assessments,\n",
    "    \"studentVle\": student_vle,\n",
    "    \"vle\": vle,\n",
    "    \"courses\": courses\n",
    "}\n",
    "\n",
    "for nombre, df in dataframes.items():\n",
    "    print(f\"\\n✅ {nombre} cargado correctamente.\")\n",
    "    print(f\"Dimensiones: {df.shape}\")\n",
    "    print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154228fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Unión y limpieza de datos\n",
    "\n",
    "# Asegurar que la columna 'score' sea numérica\n",
    "student_assessment[\"score\"] = pd.to_numeric(student_assessment[\"score\"], errors=\"coerce\")\n",
    "\n",
    "# Unir studentAssessment con assessments para obtener code_module y code_presentation\n",
    "student_assessment_full = pd.merge(student_assessment, assessments, on=\"id_assessment\", how=\"left\")\n",
    "\n",
    "# Unir studentInfo con studentRegistration\n",
    "info_reg = pd.merge(student_info, student_registration, on=[\"id_student\", \"code_module\", \"code_presentation\"], how=\"left\")\n",
    "\n",
    "# Ahora sí podemos unir con student_assessment_full\n",
    "info_reg_asess = pd.merge(info_reg, student_assessment_full, on=[\"id_student\", \"code_module\", \"code_presentation\"], how=\"left\")\n",
    "\n",
    "# Calculsar variables agregadas por estudiante\n",
    "total_assessments = student_assessment_full.groupby(\"id_student\")[\"id_assessment\"].count().reset_index(name=\"total_assessments\")\n",
    "average_score = student_assessment_full.groupby(\"id_student\")[\"score\"].mean().reset_index(name=\"average_score\")\n",
    "failed_assessments = student_assessment_full[student_assessment_full[\"score\"] < 40].groupby(\"id_student\")[\"score\"].count().reset_index(name=\"assignment_failed\")\n",
    "\n",
    "# Unir al dataset principal\n",
    "df = info_reg.drop_duplicates(subset=[\"id_student\"]).copy()\n",
    "df = pd.merge(df, total_assessments, on=\"id_student\", how=\"left\")\n",
    "df = pd.merge(df, average_score, on=\"id_student\", how=\"left\")\n",
    "df = pd.merge(df, failed_assessments, on=\"id_student\", how=\"left\")\n",
    "\n",
    "# Rellenar valores faltantes\n",
    "df[\"total_assessments\"] = df[\"total_assessments\"].fillna(0)\n",
    "df[\"average_score\"] = df[\"average_score\"].fillna(0)\n",
    "df[\"assignment_failed\"] = df[\"assignment_failed\"].fillna(0)\n",
    "\n",
    "# Validación\n",
    "print(\"✅ Unión y limpieza completadas.\")\n",
    "print(\"Dimensiones del dataset final:\", df.shape)\n",
    "df[[\"id_student\", \"highest_education\", \"final_result\", \"total_assessments\", \"average_score\", \"assignment_failed\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1. Crear variable objetivo\n",
    "# =========================\n",
    "# Crear variable binaria 'passed': 1 si final_result es 'Pass' o 'Distinction', 0 si es 'Fail' o 'Withdrawn'\n",
    "df[\"passed\"] = df[\"final_result\"].apply(lambda x: 1 if x in [\"Pass\", \"Distinction\"] else 0)\n",
    "\n",
    "# =========================\n",
    "# 2. Codificación ordinal\n",
    "# =========================\n",
    "# Codificar 'gender' como binaria\n",
    "df[\"gender\"] = df[\"gender\"].astype(str).str.strip().str.upper().map({\"M\": 0, \"F\": 1})\n",
    "\n",
    "# Codificar 'region' como ordinal según orden alfabético\n",
    "region_order = {region: idx for idx, region in enumerate(sorted(df[\"region\"].dropna().unique()))}\n",
    "df[\"region\"] = df[\"region\"].map(region_order)\n",
    "\n",
    "# Codificar 'age_band' como ordinal\n",
    "age_band_order = {\"0-35\": 0, \"35-55\": 1, \"55<=\": 2}\n",
    "df[\"age_band\"] = df[\"age_band\"].map(age_band_order)\n",
    "\n",
    "# Codificar 'highest_education' como ordinal según orden alfabético\n",
    "education_order = {edu: idx for idx, edu in enumerate(sorted(df[\"highest_education\"].dropna().unique()))}\n",
    "df[\"highest_education\"] = df[\"highest_education\"].map(education_order)\n",
    "\n",
    "# =========================\n",
    "# 3. Selección de variables\n",
    "# =========================\n",
    "features = [\n",
    "    \"gender\", \"region\", \"age_band\", \"highest_education\",\n",
    "    \"total_assessments\", \"average_score\", \"assignment_failed\"\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"passed\"]\n",
    "\n",
    "# =========================\n",
    "# 4. Eliminación de columnas innecesarias\n",
    "# =========================\n",
    "df.drop(columns=[\"imd_band\", \"final_result\", \"date_unregistration\",\"disability\"], inplace=True)\n",
    "\n",
    "# =========================\n",
    "# 5. Validación\n",
    "# =========================\n",
    "print(\"✅ Dataset preparado correctamente.\")\n",
    "print(\"Variables predictoras:\", list(X.columns))\n",
    "print(\"Distribución de la variable objetivo:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# =========================\n",
    "# 6. Guardar dataset\n",
    "# =========================\n",
    "df.to_csv(os.path.join(ruta,\"H2 - oulad_dataset_summary_vle_data.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d092ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6dc217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo Random Forest\n",
    "\n",
    "# Entrenar el modelo Random Forest\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular matriz de confusión\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "# Calcular métricas\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Resultados del modelo Random Forest:\")\n",
    "print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# crear un DataFrame para las métricas\n",
    "metrics = {\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"MSE\", \"RMSE\", \"R² Score\"],\n",
    "    \"Value\": [accuracy, precision, recall, f1, mse, rmse, r2]\n",
    "}\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "# Guardar las métricas en un archivo CSV\n",
    "df_metrics.to_csv(os.path.join(rutaMetrics_models,\"H1 - logistic_regression_metrics.csv\"), index=False)\n",
    "\n",
    "# Guardar predicciones en CSV\n",
    "results_df = pd.DataFrame({\n",
    "    \"y_test\": y_test.values,\n",
    "    \"y_pred\": y_pred\n",
    "})\n",
    "\n",
    "results_df.to_csv(os.path.join(rutaResult_Pred,\"H2 - predicciones_rf.csv\"), index=False)\n",
    "print(\"✅ Archivo 'predicciones_rf.csv' guardado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce72fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de resultados del modelo Random Forest\n",
    "\n",
    "# Matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Curva Precision-Recall\n",
    "y_scores = model.predict_proba(X_test)[:, 1]\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "# Importancia de características\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Crear figura con 3 subplots en una fila\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# Subplot 1: Matriz de Confusión\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No pasó', 'Pasó'],\n",
    "            yticklabels=['No pasó', 'Pasó'],\n",
    "            ax=axes[0])\n",
    "axes[0].set_title('Matriz de Confusión - Random Forest')\n",
    "axes[0].set_xlabel('Predicción')\n",
    "axes[0].set_ylabel('Real')\n",
    "\n",
    "# Subplot 2: Curva Precision-Recall\n",
    "axes[1].plot(recall_vals, precision_vals, label='Curva Precision-Recall', color='blue')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Curva Precision-Recall - Random Forest')\n",
    "axes[1].grid(True)\n",
    "axes[1].legend()\n",
    "\n",
    "# Subplot 3: Importancia de Características\n",
    "axes[2].bar(range(len(importances)), importances[indices], color='skyblue')\n",
    "axes[2].set_xticks(range(len(importances)))\n",
    "axes[2].set_xticklabels([features[i] for i in indices], rotation=45, ha='right')\n",
    "axes[2].set_title('Importancia de Características - Random Forest')\n",
    "axes[2].set_ylabel('Importancia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(rutaEdaImg, \"H2 - Resultados_modelo_random_forest.png\"))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de red neuronal LSTM (Long Short-Term Memory)\n",
    "\n",
    "# Preprocesamiento: escalado y reshape para LSTM\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# LSTM espera entrada 3D: (samples, timesteps, features)\n",
    "# Como no tenemos series temporales, usamos timesteps=1\n",
    "X_lstm = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# División de datos\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(\n",
    "    X_lstm, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Modelo LSTM\n",
    "model_lstm = Sequential([\n",
    "    LSTM(32, input_shape=(1, X.shape[1]), activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenamiento\n",
    "history = model_lstm.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    epochs=10, batch_size=64,\n",
    "    validation_data=(X_test_lstm, y_test_lstm),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Convertir el historial a DataFrame\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "\n",
    "# Guardar el DataFrame como CSV\n",
    "hist_df.to_csv(os.path.join(rutaMetrics_models, \"H2 - history_lstm.csv\"), index=False)\n",
    "\n",
    "# Evaluación\n",
    "loss, accuracy = model_lstm.evaluate(X_test_lstm, y_test_lstm, verbose=0)\n",
    "print(f\"✅ LSTM Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predicciones\n",
    "y_pred_lstm = (model_lstm.predict(X_test_lstm) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Predicciones probabilísticas\n",
    "y_pred_probs = model_lstm.predict(X_test_lstm)\n",
    "\n",
    "# Predicciones binarias\n",
    "y_pred_lstm = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calcular probabilidades y predicciones\n",
    "mse = mean_squared_error(y_test_lstm, y_pred_lstm)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_lstm, y_pred_lstm)\n",
    "accuracy = accuracy_score(y_test_lstm, y_pred_lstm)\n",
    "precision = precision_score(y_test_lstm, y_pred_lstm)\n",
    "recall = recall_score(y_test_lstm, y_pred_lstm)\n",
    "f1 = f1_score(y_test_lstm, y_pred_lstm)\n",
    "\n",
    "# Matriz de confusión\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_lstm, y_pred_lstm).ravel()\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Resultados del modelo LSTM:\")\n",
    "print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# crear un DataFrame para las métricas\n",
    "metrics = {\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"MSE\", \"RMSE\", \"R² Score\"],\n",
    "    \"Value\": [accuracy, precision, recall, f1, mse, rmse, r2]\n",
    "}\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "# Guardar las métricas en un archivo CSV\n",
    "df_metrics.to_csv(os.path.join(rutaMetrics_models,\"H2 - lstm_metrics.csv\"), index=False)\n",
    "\n",
    "# Guardar predicciones en CSV\n",
    "results_df = pd.DataFrame({ \n",
    "    \"y_test\": np.ravel(y_test_lstm),\n",
    "    \"y_pred\": y_pred_lstm.flatten(),\n",
    "    \"y_pred_probs\": y_pred_probs.flatten()\n",
    "})\n",
    "\n",
    "results_df.to_csv(os.path.join(rutaResult_Pred, \"H2 - predicciones_lstm.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e19e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualización de resultados del modelo LSTM\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(22, 10))\n",
    "\n",
    "# 5. Evolución de accuracy\n",
    "axes[0, 0].plot(history.history['accuracy'], label='Entrenamiento', marker='o')\n",
    "axes[0, 0].plot(history.history['val_accuracy'], label='Validación', marker='o')\n",
    "axes[0, 0].set_title('Accuracy durante el entrenamiento LSTM')\n",
    "axes[0, 0].set_xlabel('Época')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# 6. Evolución de loss\n",
    "axes[0, 1].plot(history.history['loss'], label='Entrenamiento', marker='o')\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Validación', marker='o')\n",
    "axes[0, 1].set_title('Loss durante el entrenamiento LSTM')\n",
    "axes[0, 1].set_xlabel('Época')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# 2. Curva ROC\n",
    "\n",
    "roc_auc = roc_auc_score(y_test_lstm, y_pred_probs)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test_lstm, y_pred_probs)\n",
    "axes[0, 2].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC AUC = {roc_auc:.3f}')\n",
    "axes[0, 2].plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "axes[0, 2].set_xlim([0.0, 1.0])\n",
    "axes[0, 2].set_ylim([0.0, 1.05])\n",
    "axes[0, 2].set_xlabel('False Positive Rate')\n",
    "axes[0, 2].set_ylabel('True Positive Rate')\n",
    "axes[0, 2].set_title('Curva ROC - LSTM')\n",
    "axes[0, 2].legend(loc=\"lower right\")\n",
    "\n",
    "# 4. Matriz de confusión\n",
    "conf_matrix_lstm = confusion_matrix(y_test_lstm, y_pred_lstm)\n",
    "sns.heatmap(conf_matrix_lstm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No pasó', 'Pasó'],\n",
    "            yticklabels=['No pasó', 'Pasó'],\n",
    "            ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Matriz de Confusión - LSTM')\n",
    "axes[1, 0].set_xlabel('Predicción')\n",
    "axes[1, 0].set_ylabel('Real')\n",
    "\n",
    "\n",
    "# Calcular precisión y recall\n",
    "precision, recall, _ = precision_recall_curve(y_test_lstm, y_pred_probs)\n",
    "average_precision = average_precision_score(y_test_lstm, y_pred_probs)\n",
    "\n",
    "# Insertar en Curva Precision-Recall - LSTM\n",
    "axes[1, 1].plot(recall, precision, color='blue', lw=2, label=f'AP = {average_precision:.3f}')\n",
    "axes[1, 1].set_xlabel('Recall')\n",
    "axes[1, 1].set_ylabel('Precision')\n",
    "axes[1, 1].set_title('Curva Precision-Recall - LSTM')\n",
    "axes[1, 1].legend(loc='lower left')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "# 3. Distribución de probabilidades predichas\n",
    "axes[1, 2].hist(y_pred_probs, bins=30, color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[1, 2].set_title('Distribución de Probabilidades Predichas - LSTM')\n",
    "axes[1, 2].set_xlabel('Probabilidad de pasar')\n",
    "axes[1, 2].set_ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(rutaEdaImg, \"H2 - Resultados_modelo_lstm.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c388c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model de Árbol de Decisión Optimizado\n",
    "\n",
    "# Definir el modelo base\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Definir la grilla de hiperparámetros para optimización\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Búsqueda de grilla con validación cruzada\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejor modelo encontrado\n",
    "best_dt = grid_search.best_estimator_\n",
    "\n",
    "# Predicciones\n",
    "y_pred_dt = best_dt.predict(X_test)\n",
    "\n",
    "# Métricas\n",
    "print(\"Resultados del Árbol de Decisión Optimizado:\")\n",
    "print(f\"Mejores hiperparámetros: {grid_search.best_params_}\")\n",
    "print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred_dt):.4f}\")\n",
    "\n",
    "# calcular métricas de evaluación\n",
    "mse = mean_squared_error(y_test, y_pred_dt)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred_dt)\n",
    "accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "precision = precision_score(y_test, y_pred_dt)\n",
    "recall = recall_score(y_test, y_pred_dt)\n",
    "f1 = f1_score(y_test, y_pred_dt)\n",
    "\n",
    "# crear un DataFrame para las métricas\n",
    "metrics = {\n",
    "    \"Metric\": [\"TP\",\"FP\",\"TN\",\"FN\",\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"MSE\", \"RMSE\", \"R² Score\"],\n",
    "    \"Value\": [tp, fp, tn, fn, accuracy, precision, recall, f1, mse, rmse, r2]\n",
    "}\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "# Guardar las métricas en un archivo CSV\n",
    "df_metrics.to_csv(os.path.join(rutaMetrics_models,\"H2 - decision_tree_metrics.csv\"), index=False)\n",
    "\n",
    "# Crear un DataFrame con las predicciones\n",
    "df_predictions = X_test.copy()\n",
    "df_predictions['Prediccion'] = y_pred_dt\n",
    "\n",
    "# Guardar las predicciones en un archivo CSV\n",
    "df_predictions.to_csv(os.path.join(rutaResult_Pred, \"H2 - predicciones_arbol_decision.csv\"), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c049373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de resultados del modelo de Árbol de Decisión Optimizado\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(22, 12))\n",
    "\n",
    "# 1. Matriz de confusión\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No pasó', 'Pasó'],\n",
    "            yticklabels=['No pasó', 'Pasó'],\n",
    "            ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Matriz de Confusión')\n",
    "axes[0, 0].set_xlabel('Predicción')\n",
    "axes[0, 0].set_ylabel('Real')\n",
    "\n",
    "# 2. Importancia de características\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "axes[0, 1].bar(range(len(importances)), importances[indices], color='skyblue')\n",
    "axes[0, 1].set_xticks(range(len(importances)))\n",
    "axes[0, 1].set_xticklabels([X.columns[i] for i in indices], rotation=45, ha='right')\n",
    "axes[0, 1].set_title('Importancia de Características')\n",
    "axes[0, 1].set_ylabel('Importancia')\n",
    "\n",
    "# 3. Distribución de Errores\n",
    "errores = y_test - y_pred\n",
    "axes[0, 2].hist(errores, bins=3, color='orange', edgecolor='black', rwidth=0.8)\n",
    "axes[0, 2].set_title('Distribución de Errores')\n",
    "axes[0, 2].set_xlabel('Error (y_test - y_pred)')\n",
    "axes[0, 2].set_ylabel('Frecuencia')\n",
    "axes[0, 2].set_xticks([-1, 0, 1])\n",
    "\n",
    "# 4. Curva ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "axes[1, 0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC AUC = {roc_auc:.3f}')\n",
    "axes[1, 0].plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "axes[1, 0].set_xlim([0.0, 1.0])\n",
    "axes[1, 0].set_ylim([0.0, 1.05])\n",
    "axes[1, 0].set_xlabel('False Positive Rate')\n",
    "axes[1, 0].set_ylabel('True Positive Rate')\n",
    "axes[1, 0].set_title('Curva ROC')\n",
    "axes[1, 0].legend(loc=\"lower right\")\n",
    "\n",
    "# 5. Curva Precision-Recall\n",
    "axes[1, 1].plot(recall_vals, precision_vals, label='Curva Precision-Recall', color='blue')\n",
    "axes[1, 1].set_xlabel('Recall')\n",
    "axes[1, 1].set_ylabel('Precision')\n",
    "axes[1, 1].set_title('Curva Precision-Recall')\n",
    "axes[1, 1].grid(True)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# 6. Distribución de probabilidades predichas\n",
    "axes[1, 2].hist(y_scores, bins=30, color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[1, 2].set_title('Distribución de Probabilidades Predichas')\n",
    "axes[1, 2].set_xlabel('Probabilidad de pasar')\n",
    "axes[1, 2].set_ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(rutaEdaImg, \"H2 - Resultados_modelo_decision_tree.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo de clustering KMeans(No supervisado)\n",
    "\n",
    "# Selección de variables para clustering\n",
    "clustering_features = [\"total_assessments\", \"average_score\", \"assignment_failed\"]\n",
    "X_cluster = df[clustering_features].copy()\n",
    "\n",
    "# Escalado\n",
    "scaler_cluster = StandardScaler()\n",
    "X_scaled_cluster = scaler_cluster.fit_transform(X_cluster)\n",
    "\n",
    "# KMeans clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled_cluster)\n",
    "\n",
    "# Añadir clusters al DataFrame\n",
    "df[\"cluster\"] = clusters\n",
    "\n",
    "# Evaluación (suponiendo que tienes y_test y y_pred_dt definidos)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_dt).ravel()\n",
    "\n",
    "# calcular métricas de evaluación\n",
    "mse = mean_squared_error(y_test, y_pred_dt)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred_dt)\n",
    "accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "precision = precision_score(y_test, y_pred_dt)\n",
    "recall = recall_score(y_test, y_pred_dt)\n",
    "f1 = f1_score(y_test, y_pred_dt)\n",
    "\n",
    "print(\"Resultados del modelo clustering KMeans:\")\n",
    "print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred_dt):.4f}\")\n",
    "\n",
    "# crear un DataFrame para las métricas\n",
    "metrics = {\n",
    "    \"Metric\": [\"TP\",\"FP\",\"TN\",\"FN\",\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"MSE\", \"RMSE\", \"R² Score\"],\n",
    "    \"Value\": [tp, fp, tn, fn, accuracy, precision, recall, f1, mse, rmse, r2]\n",
    "}\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "# Guardar las métricas en un archivo CSV\n",
    "df_metrics.to_csv(os.path.join(rutaMetrics_models,\"H2 - kmeans_metrics.csv\"), index=False)\n",
    "\n",
    "# Crear un DataFrame con los valores reales y predichos\n",
    "df_predictions = pd.DataFrame({\n",
    "    \"Index\": y_test.index,\n",
    "    \"Actual\": y_test.values,\n",
    "    \"Predicted\": y_pred_dt\n",
    "})\n",
    "\n",
    "# Guardar el DataFrame en un archivo CSV\n",
    "df_predictions.to_csv(os.path.join(rutaResult_Pred, \"H2 - kmeans_predictions.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430640d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de resultados y métricas modelo kmeans\n",
    "\n",
    "# Calcular métricas necesarias\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_dt).ravel()\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_dt)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_dt)\n",
    "errors = y_test - y_pred_dt\n",
    "sns.set(style=\"whitegrid\", context=\"talk\", palette=\"husl\")\n",
    "\n",
    "# Crear figura con 2 filas y 3 columnas\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Matriz de confusión\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_dt, ax=axs[0, 0])\n",
    "axs[0, 0].set_title(\"Matriz de Confusión\")\n",
    "\n",
    "# 2. Importancia de características \n",
    "try:\n",
    "    importances = model.feature_importances_\n",
    "    axs[0, 1].barh(range(len(importances)), importances)\n",
    "    axs[0, 1].set_yticks(range(len(importances)))\n",
    "    axs[0, 1].set_yticklabels(clustering_features)\n",
    "    axs[0, 1].set_title(\"Importancia de Características\")\n",
    "except:\n",
    "    axs[0, 1].text(0.5, 0.5, 'Importancia no disponible', ha='center')\n",
    "    axs[0, 1].set_title(\"Importancia de Características\")\n",
    "\n",
    "# 3. Distribución de errores\n",
    "axs[0, 2].hist(errors, bins=20, color='skyblue', edgecolor='black')\n",
    "axs[0, 2].set_title(\"Distribución de Errores\")\n",
    "axs[0, 2].set_xlabel(\"Error\")\n",
    "axs[0, 2].set_ylabel(\"Frecuencia\")\n",
    "\n",
    "# 4. Curva ROC\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc(fpr, tpr)).plot(ax=axs[1, 0])\n",
    "axs[1, 0].set_title(\"Curva ROC\")\n",
    "\n",
    "# 5. Curva Precision-Recall\n",
    "PrecisionRecallDisplay(precision=precision, recall=recall).plot(ax=axs[1, 1])\n",
    "axs[1, 1].set_title(\"Curva Precision-Recall\")\n",
    "\n",
    "# 6. Gráfico 3D de clusters KMeans\n",
    "ax3d = fig.add_subplot(2, 3, 6, projection='3d')\n",
    "scatter = ax3d.scatter(\n",
    "    df[\"total_assessments\"],\n",
    "    df[\"average_score\"],\n",
    "    df[\"assignment_failed\"],\n",
    "    c=df[\"cluster\"],\n",
    "    cmap='viridis'\n",
    ")\n",
    "ax3d.set_title(\"Clusters KMeans (3D)\")\n",
    "ax3d.set_xlabel(\"Total Assessments\")\n",
    "ax3d.set_ylabel(\"Average Score\")\n",
    "ax3d.set_zlabel(\"Assignment Failed\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
